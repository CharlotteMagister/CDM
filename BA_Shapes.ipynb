{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quarterly-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comprehensive-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, dense_diff_pool\n",
    "import torch_explain as te\n",
    "from torch_explain.logic.nn import entropy\n",
    "from torch_explain.logic.metrics import test_explanation, complexity\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from scipy.spatial.distance import cdist\n",
    "from sympy import to_dnf, lambdify\n",
    "from sklearn.metrics.cluster import homogeneity_score, completeness_score\n",
    "\n",
    "import clustering_utils\n",
    "import data_utils\n",
    "import lens_utils\n",
    "import model_utils\n",
    "import persistence_utils\n",
    "import visualisation_utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "peripheral-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "DATASET_NAME = \"BA_Shapes\"\n",
    "MODEL_NAME = f\"GCN for {DATASET_NAME}\"\n",
    "NUM_CLASSES = 4\n",
    "K = 10\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "NUM_HIDDEN_UNITS = 10\n",
    "EPOCHS = 7000\n",
    "LR = 0.001\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "NUM_NODES_VIEW = 5\n",
    "NUM_EXPANSIONS = 2\n",
    "\n",
    "LAYER_NUM = 3\n",
    "LAYER_KEY = \"conv3\"\n",
    "\n",
    "visualisation_utils.set_rc_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sufficient-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_in_features, num_hidden_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.conv0 = GCNConv(num_in_features, num_hidden_features)\n",
    "        self.conv1 = GCNConv(num_hidden_features, num_hidden_features)\n",
    "        self.conv2 = GCNConv(num_hidden_features, num_hidden_features)\n",
    "        self.conv3 = GCNConv(num_hidden_features, num_hidden_features)\n",
    "#         self.conv4 = GCNConv(num_hidden_features, num_hidden_features)\n",
    "                \n",
    "        # linear layers\n",
    "        self.lens = torch.nn.Sequential(te.nn.EntropyLinear(num_hidden_features, 1, n_classes=num_classes))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv0(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        \n",
    "#         x = self.conv4(x, edge_index)\n",
    "#         x = F.leaky_relu(x)\n",
    "                \n",
    "        self.gnn_embedding = x\n",
    "        \n",
    "        x = F.softmax(x, dim=-1)\n",
    "        x = torch.div(x, torch.max(x, dim=-1)[0].unsqueeze(1))\n",
    "        concepts = x\n",
    "        \n",
    "        x = self.lens(x)\n",
    "                \n",
    "        return concepts, x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "monthly-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(seed, path):\n",
    "    config = {'seed': seed,\n",
    "                       'dataset_name': DATASET_NAME,\n",
    "                       'model_name': MODEL_NAME,\n",
    "                       'num_classes': NUM_CLASSES,\n",
    "                       'k': K,\n",
    "                       'train_test_split': TRAIN_TEST_SPLIT,\n",
    "                       'num_hidden_units': NUM_HIDDEN_UNITS,\n",
    "                       'epochs': EPOCHS,\n",
    "                       'lr': LR,\n",
    "                       'num_nodes_view': NUM_NODES_VIEW,\n",
    "                       'num_expansions': NUM_EXPANSIONS,\n",
    "                       'layer_num': LAYER_NUM,\n",
    "                       'layer_key': LAYER_KEY\n",
    "                      }\n",
    "        \n",
    "    # load data\n",
    "    G, labels = data_utils.load_syn_data(DATASET_NAME)\n",
    "    data = data_utils.prepare_syn_data(G, labels, TRAIN_TEST_SPLIT)\n",
    "\n",
    "    # model training\n",
    "    model = GCN(data[\"x\"].shape[1], NUM_HIDDEN_UNITS, NUM_CLASSES)\n",
    "    \n",
    "    # register hooks to track activation\n",
    "    model = model_utils.register_hooks(model)\n",
    "    \n",
    "    # train \n",
    "    start_time = time.time()\n",
    "    train_acc, test_acc, train_loss, test_loss = model_utils.train(model, data, EPOCHS, LR)\n",
    "    print(\"End time \", (time.time() - start_time))\n",
    "        \n",
    "#     visualisation_utils.plot_model_accuracy(train_acc, test_acc, MODEL_NAME, path)\n",
    "#     visualisation_utils.plot_model_loss(train_loss, test_loss, MODEL_NAME, path)\n",
    "    \n",
    "#     x = data[\"x\"]\n",
    "#     edges = data['edges']\n",
    "#     edges_t = data['edge_list'].numpy()\n",
    "#     y = data[\"y\"]\n",
    "#     train_mask = data[\"train_mask\"]\n",
    "#     test_mask = data[\"test_mask\"]\n",
    "    \n",
    "#     # get model activations for complete dataset\n",
    "#     concepts, _ = model(x, edges)\n",
    "#     activation = torch.squeeze(model_utils.activation_list[LAYER_KEY]).detach().numpy()\n",
    "    \n",
    "#     # find centroids\n",
    "#     centroids, centroid_labels, used_centroid_labels = clustering_utils.find_centroids(activation, concepts, y)\n",
    "#     print(f\"Number of cenroids: {len(centroids)}\")\n",
    "    \n",
    "#     # plot concept heatmaps\n",
    "#     visualisation_utils.plot_concept_heatmap(centroids, activation, y, used_centroid_labels, MODEL_NAME, LAYER_NUM, path)\n",
    "    \n",
    "#     # concept alignment\n",
    "#     homogeneity = homogeneity_score(y, used_centroid_labels)\n",
    "#     # clustering efficency\n",
    "#     completeness = completeness_score(y, used_centroid_labels)\n",
    "    \n",
    "#     print(f\"Concept homogeneity score: {homogeneity}\")\n",
    "#     print(f\"Concept completeness score: {completeness}\")\n",
    "    \n",
    "#     # generate explanations\n",
    "#     explanations = lens_utils.explain_classes(model, concepts, y, train_mask, test_mask)\n",
    "    \n",
    "#     # plot clustering\n",
    "#     visualisation_utils.plot_clustering(seed, activation, y, centroids, centroid_labels, used_centroid_labels, MODEL_NAME, LAYER_NUM, path)\n",
    "    \n",
    "#     # calculate cluster sizing\n",
    "#     cluster_counts = visualisation_utils.print_cluster_counts(used_centroid_labels)\n",
    "\n",
    "#     # plot samples\n",
    "#     sample_graphs, sample_feat = visualisation_utils.plot_samples(None, activation, y, LAYER_NUM, len(centroids), \"Differential Clustering\", \"Raw\", NUM_NODES_VIEW, edges_t, NUM_EXPANSIONS, path, concepts=centroids)\n",
    "    \n",
    "#     # dump data\n",
    "#     persistence_utils.persist_experiment(config, path, 'config.z')\n",
    "#     persistence_utils.persist_experiment(data, path,'data.z')\n",
    "    \n",
    "#     persistence_utils.persist_model(model, path, 'model.z')\n",
    "    \n",
    "#     persistence_utils.persist_experiment(concepts, path, 'concepts.z')\n",
    "#     persistence_utils.persist_experiment(model_utils.activation_list, path, 'activation_list.z')\n",
    "#     persistence_utils.persist_experiment(centroids, path, 'centroids.z')\n",
    "#     persistence_utils.persist_experiment(centroid_labels, path, 'centroid_labels.z')\n",
    "#     persistence_utils.persist_experiment(used_centroid_labels, path, 'used_centroid_labels.z')\n",
    "    \n",
    "#     concept_metrics = [('homogeneity', homogeneity), ('completeness', completeness), ('cluster_count', cluster_counts)]\n",
    "#     persistence_utils.persist_experiment(concept_metrics, path, 'concept_metrics.z')\n",
    "#     persistence_utils.persist_experiment(explanations, path, 'explanations.z')\n",
    "#     persistence_utils.persist_experiment(sample_graphs, path, 'sample_graphs.z')\n",
    "#     persistence_utils.persist_experiment(sample_feat, path, 'sample_feat.z')\n",
    "    \n",
    "    # clean up\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "corrected-fever",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "START EXPERIMENT-----------------------------------------\n",
      "\n",
      "Task: Node Classification\n",
      "Number of features:  700\n",
      "Number of labels:  700\n",
      "Number of classes:  700\n",
      "Number of edges:  2\n",
      "Epoch: 6994, Loss: 0.05727, Train Acc: 0.98561, Test Acc: 0.99306\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time  67.5832929611206, Train Acc: 0.98561, Test Acc: 0.99306\n",
      "\n",
      "END EXPERIMENT-------------------------------------------\n",
      "\n",
      "\n",
      "START EXPERIMENT-----------------------------------------\n",
      "\n",
      "Task: Node Classification\n",
      "Number of features:  700\n",
      "Number of labels:  700\n",
      "Number of classes:  700\n",
      "Number of edges:  2\n",
      "Epoch: 6979, Loss: 0.15618, Train Acc: 0.96174, Test Acc: 0.97600\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time  66.6586070060737, Train Acc: 0.96174, Test Acc: 0.97600\n",
      "\n",
      "END EXPERIMENT-------------------------------------------\n",
      "\n",
      "\n",
      "START EXPERIMENT-----------------------------------------\n",
      "\n",
      "Task: Node Classification\n",
      "Number of features:  700\n",
      "Number of labels:  700\n",
      "Number of classes:  700\n",
      "Number of edges:  2\n",
      "Epoch: 6986, Loss: 0.06039, Train Acc: 0.97695, Test Acc: 0.97059\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time  67.16350102424622 Train Acc: 0.97695, Test Acc: 0.97059\n",
      "\n",
      "END EXPERIMENT-------------------------------------------\n",
      "\n",
      "\n",
      "START EXPERIMENT-----------------------------------------\n",
      "\n",
      "Task: Node Classification\n",
      "Number of features:  700\n",
      "Number of labels:  700\n",
      "Number of classes:  700\n",
      "Number of edges:  2\n",
      "Epoch: 6980, Loss: 0.05085, Train Acc: 0.99113, Test Acc: 0.98529\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time  64.20462012290955 Train Acc: 0.99113, Test Acc: 0.98529\n",
      "\n",
      "END EXPERIMENT-------------------------------------------\n",
      "\n",
      "\n",
      "START EXPERIMENT-----------------------------------------\n",
      "\n",
      "Task: Node Classification\n",
      "Number of features:  700\n",
      "Number of labels:  700\n",
      "Number of classes:  700\n",
      "Number of edges:  2\n",
      "End time  72.23372006416321 Train Acc: 0.98162, Test Acc: 0.98077\n",
      "\n",
      "END EXPERIMENT-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run multiple times for confidence interval - seeds generated using Google's random number generator\n",
    "random_seeds = [42, 19, 76, 58, 92]\n",
    "\n",
    "for seed in random_seeds:\n",
    "    print(\"\\nSTART EXPERIMENT-----------------------------------------\\n\")\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    path = os.path.join(\"..\", \"output\", DATASET_NAME, f\"seed_{seed}_v2\")\n",
    "    data_utils.create_path(path)\n",
    "\n",
    "    run_experiment(seed, path)\n",
    "    \n",
    "    print(\"\\nEND EXPERIMENT-------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-reward",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newron_len_env",
   "language": "python",
   "name": "newron_len_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
