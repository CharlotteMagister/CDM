{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import Sequential, GCNConv, dense_diff_pool\n",
    "import torch_explain as te\n",
    "from torch_explain.logic.nn import entropy\n",
    "from torch_explain.logic.metrics import test_explanation, complexity\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from scipy.spatial.distance import cdist\n",
    "from sympy import to_dnf, lambdify\n",
    "from sklearn.metrics.cluster import homogeneity_score, completeness_score\n",
    "\n",
    "import clustering_utils\n",
    "import data_utils\n",
    "import lens_utils\n",
    "import model_utils\n",
    "import persistence_utils\n",
    "import visualisation_utils\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "DATASET_NAME = \"Mutagenicity\"\n",
    "MODEL_NAME = f\"GCN for {DATASET_NAME}\"\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "NUM_HIDDEN_UNITS = 40\n",
    "EPOCHS = 1000\n",
    "LR = 0.001\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "NUM_NODES_VIEW = 5\n",
    "NUM_EXPANSIONS = 4\n",
    "\n",
    "LAYER_NUM = 3\n",
    "LAYER_KEY = \"conv3\"\n",
    "\n",
    "visualisation_utils.set_rc_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_in_features, num_hidden_features, cluster_encoding_size, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.conv0 = GCNConv(num_in_features, num_hidden_features)\n",
    "        self.conv1 = GCNConv(num_hidden_features, num_hidden_features)\n",
    "        self.conv2 = GCNConv(num_hidden_features, num_hidden_features)\n",
    "        self.conv3 = GCNConv(num_hidden_features, cluster_encoding_size)\n",
    "\n",
    "        self.pool = model_utils.Pool()\n",
    "\n",
    "        # linear layers\n",
    "        self.lens = torch.nn.Sequential(te.nn.EntropyLinear(cluster_encoding_size, 1, n_classes=num_classes))\n",
    "#         self.lens = nn.Linear(cluster_encoding_size, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv0(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        \n",
    "        x = x.squeeze()\n",
    "    \n",
    "        self.gnn_node_embedding = x\n",
    "        \n",
    "        x = F.softmax(x, dim=-1)\n",
    "        x = torch.div(x, torch.max(x, dim=-1)[0].unsqueeze(1))\n",
    "        self.gnn_node_concepts = x\n",
    "        \n",
    "        x = self.pool(x, batch)\n",
    "        self.gnn_graph_concepts = x\n",
    "        \n",
    "        x = self.lens(x)\n",
    "                \n",
    "        return self.gnn_node_concepts, x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(seed, path):\n",
    "    config = {'seed': seed,\n",
    "                       'dataset_name': DATASET_NAME,\n",
    "                       'model_name': MODEL_NAME,\n",
    "                       'num_classes': NUM_CLASSES,\n",
    "                       'train_test_split': TRAIN_TEST_SPLIT,\n",
    "                       'batch_size': BATCH_SIZE,\n",
    "                       'num_hidden_units': NUM_HIDDEN_UNITS,\n",
    "                       'cluster_encoding_size': CLUSTER_ENCODING_SIZE,\n",
    "                       'epochs': EPOCHS,\n",
    "                       'lr': LR,\n",
    "                       'num_nodes_view': NUM_NODES_VIEW,\n",
    "                       'num_expansions': NUM_EXPANSIONS,\n",
    "                       'layer_num': LAYER_NUM,\n",
    "                       'layer_key': LAYER_KEY\n",
    "                      }\n",
    "    persistence_utils.persist_experiment(config, path, 'config.z')\n",
    "\n",
    "    # load data\n",
    "    graphs = data_utils.load_real_data(DATASET_NAME)\n",
    "    train_loader, test_loader, full_train_loader, full_test_loader, full_loader, small_loader = data_utils.prepare_real_data(graphs, TRAIN_TEST_SPLIT, BATCH_SIZE, DATASET_NAME)\n",
    "\n",
    "    # model training\n",
    "    model = GCN(graphs.num_node_features, NUM_HIDDEN_UNITS, CLUSTER_ENCODING_SIZE, graphs.num_classes)\n",
    "    \n",
    "    # register hooks to track activation\n",
    "    model = model_utils.register_hooks(model)\n",
    "    \n",
    "    # train \n",
    "    train_acc, test_acc, train_loss, test_loss = model_utils.train_graph_class(model, train_loader, test_loader, full_loader, EPOCHS, LR, if_interpretable_model=True)\n",
    "    persistence_utils.persist_model(model, path, 'model.z')\n",
    "\n",
    "    visualisation_utils.plot_model_accuracy(train_acc, test_acc, MODEL_NAME, path)\n",
    "    visualisation_utils.plot_model_loss(train_loss, test_loss, MODEL_NAME, path)\n",
    "    \n",
    "    # get model activations for complete dataset\n",
    "    train_data = next(iter(full_train_loader))\n",
    "    train_node_concepts, _ = model(train_data.x, train_data.edge_index, train_data.batch)\n",
    "    train_node_activation = model.gnn_node_embedding\n",
    "    train_graph_concepts = model.gnn_graph_concepts\n",
    "    train_graph_activation = model.gnn_graph_concepts\n",
    "    \n",
    "    test_data = next(iter(full_test_loader))\n",
    "    test_node_concepts, _ = model(test_data.x, test_data.edge_index, test_data.batch)\n",
    "    test_node_activation = model.gnn_node_embedding\n",
    "    test_graph_concepts = model.gnn_graph_concepts\n",
    "    test_graph_activation = model.gnn_graph_concepts\n",
    "    \n",
    "    node_concepts = torch.vstack((train_node_concepts, test_node_concepts))\n",
    "    node_activation = torch.vstack((train_node_activation, test_node_activation)).detach().numpy()\n",
    "    graph_concepts = torch.vstack([train_graph_concepts, test_graph_concepts])\n",
    "    graph_activation = torch.vstack((train_graph_activation, test_graph_activation)).detach().numpy()\n",
    "\n",
    "    y = torch.cat((train_data.y, test_data.y))\n",
    "    expanded_train_y = data_utils.reshape_graph_to_node_data(train_data.y, train_data.batch)\n",
    "    expanded_test_y = data_utils.reshape_graph_to_node_data(test_data.y, test_data.batch)\n",
    "    expanded_y = torch.cat((expanded_train_y, expanded_test_y))\n",
    "    \n",
    "    train_mask = np.zeros(y.shape[0], dtype=bool)\n",
    "    train_mask[:train_data.y.shape[0]] = True\n",
    "    test_mask = ~train_mask\n",
    "    \n",
    "    offset = train_data.batch[-1] + 1\n",
    "    batch = torch.cat((train_data.batch, test_data.batch + offset))\n",
    "    \n",
    "    persistence_utils.persist_experiment(node_concepts, path, 'node_concepts.z')\n",
    "    persistence_utils.persist_experiment(node_activation, path, 'node_activation.z')\n",
    "    persistence_utils.persist_experiment(node_concepts, path, 'graph_concepts.z')\n",
    "    persistence_utils.persist_experiment(node_activation, path, 'graph_activation.z')\n",
    "        \n",
    "    # generate explanations using LENS\n",
    "    explanations = lens_utils.explain_classes(model, graph_concepts, y, train_mask, test_mask)\n",
    "    persistence_utils.persist_experiment(explanations, path, 'explanations.z')\n",
    "    \n",
    "    print(\"\\n_____________THIS IS FOR NODES_____________\")\n",
    "    # find centroids\n",
    "    centroids, centroid_labels, used_centroid_labels = clustering_utils.find_centroids(node_activation, node_concepts, expanded_y)\n",
    "    persistence_utils.persist_experiment(centroids, path, 'node_centroids.z')\n",
    "    persistence_utils.persist_experiment(centroid_labels, path, 'node_centroid_labels.z')\n",
    "    persistence_utils.persist_experiment(used_centroid_labels, path, 'node_used_centroid_labels.z')\n",
    "    print(f\"Number of node cenroids: {len(centroids)}\")\n",
    "\n",
    "    # calculate cluster sizing\n",
    "    cluster_counts = visualisation_utils.print_cluster_counts(used_centroid_labels)\n",
    "\n",
    "    # concept alignment\n",
    "    homogeneity = homogeneity_score(expanded_y, used_centroid_labels)\n",
    "    \n",
    "    # clustering efficency\n",
    "    completeness = completeness_score(expanded_y, used_centroid_labels)\n",
    "    \n",
    "    print(f\"Concept homogeneity score: {homogeneity}\")\n",
    "    print(f\"Concept completeness score: {completeness}\")\n",
    "    concept_metrics = [('homogeneity', homogeneity), ('completeness', completeness), ('cluster_count', cluster_counts)]\n",
    "    persistence_utils.persist_experiment(concept_metrics, path, 'node_concept_metrics.z')\n",
    "    \n",
    "    # plot concept heatmaps\n",
    "    visualisation_utils.plot_concept_heatmap(centroids, node_concepts, expanded_y, used_centroid_labels, MODEL_NAME, LAYER_NUM, path, id_title=\"Node \", id_path=\"node_\")\n",
    "    \n",
    "    # plot clustering - REDUCING DATA TO TRAINING SET\n",
    "    test_node_activation = test_node_activation.detach().numpy()\n",
    "    expanded_test_mask = data_utils.reshape_graph_to_node_data(test_mask, batch)\n",
    "    node_labels = []\n",
    "    for x, batch_idx in zip(test_data.x, test_data.batch):\n",
    "        node_labels.append(np.argmax(x.detach().numpy(), axis=0))\n",
    "    \n",
    "    _, centroid_labels, _ = clustering_utils.find_centroids(test_node_activation, node_concepts[expanded_test_mask], expanded_y[expanded_test_mask])\n",
    "    print(\"Nodes to visualise \", expanded_test_mask.shape)\n",
    "\n",
    "    visualisation_utils.plot_clustering(seed, test_node_activation, expanded_test_y, centroids, centroid_labels, used_centroid_labels[expanded_test_mask], MODEL_NAME, LAYER_NUM, path, id_title=\"Node \", id_path=\"_node\")\n",
    "\n",
    "    # plot samples\n",
    "    edges_t = test_data.edge_index.transpose(0, 1).detach().numpy()\n",
    "    sample_graphs, sample_feat = visualisation_utils.plot_samples(None, test_node_activation, expanded_test_y, LAYER_NUM, len(centroids), \"Differential Clustering\", \"Raw\", NUM_NODES_VIEW, edges_t, NUM_EXPANSIONS, path, concepts=centroids, graph_data=node_labels, graph_name=\"Mutagenicity\")\n",
    "    persistence_utils.persist_experiment(sample_graphs, path, 'node_sample_graphs.z')\n",
    "    persistence_utils.persist_experiment(sample_feat, path, 'node_sample_feat.z')\n",
    "    \n",
    "    \n",
    "    print(\"\\n_____________THIS IS FOR GRAPHS____________\")\n",
    "    \n",
    "    # find centroids\n",
    "    centroids, centroid_labels, used_centroid_labels = clustering_utils.find_centroids(graph_activation, graph_concepts, y)\n",
    "    print(f\"Number of graph cenroids: {len(centroids)}\")\n",
    "    persistence_utils.persist_experiment(centroids, path, 'graph_centroids.z')\n",
    "    persistence_utils.persist_experiment(centroid_labels, path, 'graph_centroid_labels.z')\n",
    "    persistence_utils.persist_experiment(used_centroid_labels, path, 'graph_used_centroid_labels.z')\n",
    "   \n",
    "    # calculate cluster sizing\n",
    "    cluster_counts = visualisation_utils.print_cluster_counts(used_centroid_labels)\n",
    "\n",
    "    # concept alignment\n",
    "    homogeneity = homogeneity_score(y, used_centroid_labels)\n",
    "    \n",
    "    # clustering efficency\n",
    "    completeness = completeness_score(y, used_centroid_labels)\n",
    "    \n",
    "    print(f\"Concept homogeneity score: {homogeneity}\")\n",
    "    print(f\"Concept completeness score: {completeness}\")\n",
    "    concept_metrics = [('homogeneity', homogeneity), ('completeness', completeness), ('cluster_count', cluster_counts)]\n",
    "    persistence_utils.persist_experiment(concept_metrics, path, 'graph_concept_metrics.z')  \n",
    "    \n",
    "    # plot concept heatmaps\n",
    "    visualisation_utils.plot_concept_heatmap(centroids, graph_concepts, y, used_centroid_labels, MODEL_NAME, LAYER_NUM, path, id_title=\"Graph \", id_path=\"graph_\")\n",
    "    \n",
    "    # plot clustering\n",
    "    visualisation_utils.plot_clustering(seed, graph_activation, y, centroids, centroid_labels, used_centroid_labels, MODEL_NAME, LAYER_NUM, path, id_title=\"Graph \", id_path=\"_graph\")\n",
    "\n",
    "    # clean up\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-explosion",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run multiple times for confidence interval - seeds generated using Google's random number generator\n",
    "random_seeds = [42, 19, 76, 58, 92]\n",
    "\n",
    "for seed in random_seeds:\n",
    "    print(\"\\nSTART EXPERIMENT-----------------------------------------\\n\")\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    path = os.path.join(\"..\", \"output\", DATASET_NAME, f\"seed_{seed}\")\n",
    "    data_utils.create_path(path)\n",
    "\n",
    "    run_experiment(seed, path)\n",
    "    \n",
    "    print(\"\\nEND EXPERIMENT-------------------------------------------\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-music",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-hardwood",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
